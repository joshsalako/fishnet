{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1_8X1ALLmO4tMc9aw0uDNqzjAX7jOL-_d",
      "authorship_tag": "ABX9TyPrFRo041kNmJgo8Y8A8U6h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshsalako/fishnet/blob/main/fishnet_retinanet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vpzpf-RScKhS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.detection import retinanet_resnet50_fpn\n",
        "from torchvision.datasets import VOCDetection\n",
        "import torchvision.transforms as T\n",
        "from torch.optim import SGD, Adam\n",
        "from torchvision.models.detection.retinanet import RetinaNetHead\n",
        "from torchvision.datasets.vision import StandardTransform\n",
        "import torchvision\n",
        "from torchvision.transforms import functional as F\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from torchvision.datasets.coco import CocoDetection\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import csv\n",
        "import albumentations as A\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = VOCDetection(root='/content/drive/MyDrive/Catfish',\n",
        "                             year='2007',\n",
        "                             image_set='train',\n",
        "                             #transform=transform,\n",
        "                             download=False)\n",
        "val_dataset = VOCDetection(root='/content/drive/MyDrive/Catfish',\n",
        "                             year='2007',\n",
        "                             image_set='val',\n",
        "                             #transform=transform,\n",
        "                             download=False)"
      ],
      "metadata": {
        "id": "xgR9nI-H6FlN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_int = {'fish': 1}  # Adjust this mapping to include all your labels\n",
        "\n",
        "def custom_collate(batch):\n",
        "    processed_images = []\n",
        "    processed_targets = []\n",
        "\n",
        "    transform = A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.Rotate(limit=15, p=0.5),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
        "\n",
        "    for img, target in batch:\n",
        "        img = np.array(img)  # Convert PIL Image to numpy array\n",
        "        boxes = boxes = [[float(coord) for coord in obj['bndbox'].values()] for obj in target['annotation']['object']]\n",
        "        labels = [label_to_int[obj['name']] for obj in target['annotation']['object']]\n",
        "        transformed = transform(image=img, bboxes=boxes, class_labels=labels)\n",
        "        processed_images.append(transformed['image'])\n",
        "        processed_targets.append({\n",
        "            'boxes': torch.tensor(transformed['bboxes'], dtype=torch.float32),\n",
        "            'labels': torch.tensor(transformed['class_labels'], dtype=torch.long)\n",
        "        })\n",
        "\n",
        "    return torch.stack(processed_images), processed_targets\n",
        "\n",
        "def custom_collate_val(batch):\n",
        "    processed_images = []\n",
        "    processed_targets = []\n",
        "\n",
        "    transform = A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        #A.HorizontalFlip(p=0.5),\n",
        "        #A.VerticalFlip(p=0.5),\n",
        "        #A.Rotate(limit=30, p=0.5),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
        "\n",
        "    for img, target in batch:\n",
        "        img = np.array(img)  # Convert PIL Image to numpy array\n",
        "        boxes = boxes = [[float(coord) for coord in obj['bndbox'].values()] for obj in target['annotation']['object']]\n",
        "        labels = [label_to_int[obj['name']] for obj in target['annotation']['object']]\n",
        "        transformed = transform(image=img, bboxes=boxes, class_labels=labels)\n",
        "        processed_images.append(transformed['image'])\n",
        "        processed_targets.append({\n",
        "            'boxes': torch.tensor(transformed['bboxes'], dtype=torch.float32),\n",
        "            'labels': torch.tensor(transformed['class_labels'], dtype=torch.long)\n",
        "        })\n",
        "\n",
        "    return torch.stack(processed_images), processed_targets"
      ],
      "metadata": {
        "id": "RhiwFZUPfnCL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=custom_collate)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=custom_collate_val)"
      ],
      "metadata": {
        "id": "0s7KbLOCZYeE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the pre-trained RetinaNet model\n",
        "model = retinanet_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Step 3: Modify the model for your specific task\n",
        "num_classes = 2  # Get number of classes from VOC dataset\n",
        "\n",
        "# Create a new RetinaNet head with the correct number of classes\n",
        "new_head = RetinaNetHead(\n",
        "    in_channels=model.backbone.out_channels,\n",
        "    num_anchors=model.head.classification_head.num_anchors,\n",
        "    num_classes=num_classes  # New number of classes\n",
        ")\n",
        "\n",
        "# Replace the existing head with the new head\n",
        "model.head = new_head"
      ],
      "metadata": {
        "id": "niI142yLdLrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c9681e-f416-4713-cb99-ade30d4ba083"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=RetinaNet_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_summary(model):\n",
        "    print(\"Model Summary:\\n\")\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"Layer: {name} | Size: {param.size()} | Trainable: {param.requires_grad}\")\n",
        "\n",
        "print_model_summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp4A7tQHC9BX",
        "outputId": "fc16d17d-7dac-41c5-d8f0-40341f4081dd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Summary:\n",
            "\n",
            "Layer: backbone.body.conv1.weight | Size: torch.Size([64, 3, 7, 7]) | Trainable: False\n",
            "Layer: backbone.body.layer1.0.conv1.weight | Size: torch.Size([64, 64, 1, 1]) | Trainable: False\n",
            "Layer: backbone.body.layer1.0.conv2.weight | Size: torch.Size([64, 64, 3, 3]) | Trainable: False\n",
            "Layer: backbone.body.layer1.0.conv3.weight | Size: torch.Size([256, 64, 1, 1]) | Trainable: False\n",
            "Layer: backbone.body.layer1.0.downsample.0.weight | Size: torch.Size([256, 64, 1, 1]) | Trainable: False\n",
            "Layer: backbone.body.layer1.1.conv1.weight | Size: torch.Size([64, 256, 1, 1]) | Trainable: False\n",
            "Layer: backbone.body.layer1.1.conv2.weight | Size: torch.Size([64, 64, 3, 3]) | Trainable: False\n",
            "Layer: backbone.body.layer1.1.conv3.weight | Size: torch.Size([256, 64, 1, 1]) | Trainable: False\n",
            "Layer: backbone.body.layer1.2.conv1.weight | Size: torch.Size([64, 256, 1, 1]) | Trainable: False\n",
            "Layer: backbone.body.layer1.2.conv2.weight | Size: torch.Size([64, 64, 3, 3]) | Trainable: False\n",
            "Layer: backbone.body.layer1.2.conv3.weight | Size: torch.Size([256, 64, 1, 1]) | Trainable: False\n",
            "Layer: backbone.body.layer2.0.conv1.weight | Size: torch.Size([128, 256, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer2.0.conv2.weight | Size: torch.Size([128, 128, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer2.0.conv3.weight | Size: torch.Size([512, 128, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer2.0.downsample.0.weight | Size: torch.Size([512, 256, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer2.1.conv1.weight | Size: torch.Size([128, 512, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer2.1.conv2.weight | Size: torch.Size([128, 128, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer2.1.conv3.weight | Size: torch.Size([512, 128, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer2.2.conv1.weight | Size: torch.Size([128, 512, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer2.2.conv2.weight | Size: torch.Size([128, 128, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer2.2.conv3.weight | Size: torch.Size([512, 128, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer2.3.conv1.weight | Size: torch.Size([128, 512, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer2.3.conv2.weight | Size: torch.Size([128, 128, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer2.3.conv3.weight | Size: torch.Size([512, 128, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.0.conv1.weight | Size: torch.Size([256, 512, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.0.conv2.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer3.0.conv3.weight | Size: torch.Size([1024, 256, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.0.downsample.0.weight | Size: torch.Size([1024, 512, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.1.conv1.weight | Size: torch.Size([256, 1024, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.1.conv2.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer3.1.conv3.weight | Size: torch.Size([1024, 256, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.2.conv1.weight | Size: torch.Size([256, 1024, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.2.conv2.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer3.2.conv3.weight | Size: torch.Size([1024, 256, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.3.conv1.weight | Size: torch.Size([256, 1024, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.3.conv2.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer3.3.conv3.weight | Size: torch.Size([1024, 256, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.4.conv1.weight | Size: torch.Size([256, 1024, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.4.conv2.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer3.4.conv3.weight | Size: torch.Size([1024, 256, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.5.conv1.weight | Size: torch.Size([256, 1024, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer3.5.conv2.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer3.5.conv3.weight | Size: torch.Size([1024, 256, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer4.0.conv1.weight | Size: torch.Size([512, 1024, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer4.0.conv2.weight | Size: torch.Size([512, 512, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer4.0.conv3.weight | Size: torch.Size([2048, 512, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer4.0.downsample.0.weight | Size: torch.Size([2048, 1024, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer4.1.conv1.weight | Size: torch.Size([512, 2048, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer4.1.conv2.weight | Size: torch.Size([512, 512, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer4.1.conv3.weight | Size: torch.Size([2048, 512, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer4.2.conv1.weight | Size: torch.Size([512, 2048, 1, 1]) | Trainable: True\n",
            "Layer: backbone.body.layer4.2.conv2.weight | Size: torch.Size([512, 512, 3, 3]) | Trainable: True\n",
            "Layer: backbone.body.layer4.2.conv3.weight | Size: torch.Size([2048, 512, 1, 1]) | Trainable: True\n",
            "Layer: backbone.fpn.inner_blocks.0.0.weight | Size: torch.Size([256, 512, 1, 1]) | Trainable: True\n",
            "Layer: backbone.fpn.inner_blocks.0.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: backbone.fpn.inner_blocks.1.0.weight | Size: torch.Size([256, 1024, 1, 1]) | Trainable: True\n",
            "Layer: backbone.fpn.inner_blocks.1.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: backbone.fpn.inner_blocks.2.0.weight | Size: torch.Size([256, 2048, 1, 1]) | Trainable: True\n",
            "Layer: backbone.fpn.inner_blocks.2.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: backbone.fpn.layer_blocks.0.0.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: backbone.fpn.layer_blocks.0.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: backbone.fpn.layer_blocks.1.0.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: backbone.fpn.layer_blocks.1.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: backbone.fpn.layer_blocks.2.0.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: backbone.fpn.layer_blocks.2.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: backbone.fpn.extra_blocks.p6.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: backbone.fpn.extra_blocks.p6.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: backbone.fpn.extra_blocks.p7.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: backbone.fpn.extra_blocks.p7.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: head.classification_head.conv.0.0.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: head.classification_head.conv.0.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: head.classification_head.conv.1.0.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: head.classification_head.conv.1.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: head.classification_head.conv.2.0.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: head.classification_head.conv.2.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: head.classification_head.conv.3.0.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: head.classification_head.conv.3.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: head.classification_head.cls_logits.weight | Size: torch.Size([18, 256, 3, 3]) | Trainable: True\n",
            "Layer: head.classification_head.cls_logits.bias | Size: torch.Size([18]) | Trainable: True\n",
            "Layer: head.regression_head.conv.0.0.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: head.regression_head.conv.0.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: head.regression_head.conv.1.0.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: head.regression_head.conv.1.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: head.regression_head.conv.2.0.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: head.regression_head.conv.2.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: head.regression_head.conv.3.0.weight | Size: torch.Size([256, 256, 3, 3]) | Trainable: True\n",
            "Layer: head.regression_head.conv.3.0.bias | Size: torch.Size([256]) | Trainable: True\n",
            "Layer: head.regression_head.bbox_reg.weight | Size: torch.Size([36, 256, 3, 3]) | Trainable: True\n",
            "Layer: head.regression_head.bbox_reg.bias | Size: torch.Size([36]) | Trainable: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "OllPkjLQCIj6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "r4tNuNwu0ilZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "dhW5O-JbMwxM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train the model\n",
        "optimizer = Adam(model.parameters(), lr=0.005, weight_decay=0.0005)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "num_epochs = 60\n",
        "min_val_loss = float('inf')\n",
        "\n",
        "# Check for GPU availability and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the device\n",
        "model.to(device)\n",
        "\n",
        "# Create a CSV file to log the losses\n",
        "with open('/content/drive/MyDrive/Catfish/retina_loss_log.csv', mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Epoch', 'Training Loss', 'Validation Loss'])\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for images, targets in train_loader:\n",
        "            # Move data to the device\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            #print(losses)\n",
        "            losses.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += losses.item()\n",
        "\n",
        "        # Calculate average training loss\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        #model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, targets in val_loader:\n",
        "                # Move data to the device\n",
        "                images = list(image.to(device) for image in images)\n",
        "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "                loss_dict = model(images, targets)\n",
        "                #print(loss_dict)\n",
        "                val_loss += sum(loss.item() for loss in loss_dict.values())\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        val_loss /= len(val_loader)\n",
        "        # Step the scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < min_val_loss:\n",
        "        # Update the minimum validation loss\n",
        "            min_val_loss = val_loss\n",
        "            # Save the model\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/Catfish/retinanet_model.pth')\n",
        "            print(f'Saving model at epoch {epoch} with Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Log losses to the CSV file\n",
        "        writer.writerow([epoch, train_loss, val_loss])\n",
        "\n",
        "        # Print losses\n",
        "        print(f'Epoch {epoch}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "qIzAd-ZxdeoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d71babf-e591-4505-adf0-286664edf20d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model at epoch 0 with Validation Loss: 1.6081\n",
            "Epoch 0, Training Loss: 4894.4873, Validation Loss: 1.6081\n",
            "Epoch 1, Training Loss: 3.0493, Validation Loss: 34.1506\n",
            "Epoch 2, Training Loss: 3.8750, Validation Loss: 1.7290\n",
            "Epoch 3, Training Loss: 3.5399, Validation Loss: 1.6596\n",
            "Epoch 4, Training Loss: 1.7127, Validation Loss: 1.8305\n",
            "Saving model at epoch 5 with Validation Loss: 1.4522\n",
            "Epoch 5, Training Loss: 1.7565, Validation Loss: 1.4522\n",
            "Epoch 6, Training Loss: 1.4486, Validation Loss: 1.6702\n",
            "Saving model at epoch 7 with Validation Loss: 1.1789\n",
            "Epoch 7, Training Loss: 1.5855, Validation Loss: 1.1789\n",
            "Epoch 8, Training Loss: 5.9440, Validation Loss: 1.7303\n",
            "Epoch 9, Training Loss: 4.8131, Validation Loss: 1.3185\n",
            "Epoch 10, Training Loss: 1.7355, Validation Loss: 1.3078\n",
            "Epoch 11, Training Loss: 1.2591, Validation Loss: 1.8390\n",
            "Epoch 12, Training Loss: 1.3315, Validation Loss: 1.3748\n",
            "Epoch 00014: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 13, Training Loss: 1.3985, Validation Loss: 2.2773\n",
            "Saving model at epoch 14 with Validation Loss: 1.0921\n",
            "Epoch 14, Training Loss: 1.5067, Validation Loss: 1.0921\n",
            "Epoch 15, Training Loss: 1.0955, Validation Loss: 1.1675\n",
            "Saving model at epoch 16 with Validation Loss: 1.0902\n",
            "Epoch 16, Training Loss: 1.0527, Validation Loss: 1.0902\n",
            "Epoch 17, Training Loss: 1.0601, Validation Loss: 1.1027\n",
            "Epoch 18, Training Loss: 1.0653, Validation Loss: 1.1460\n",
            "Epoch 19, Training Loss: 1.0307, Validation Loss: 1.1221\n",
            "Epoch 20, Training Loss: 1.0415, Validation Loss: 1.2063\n",
            "Saving model at epoch 21 with Validation Loss: 1.0858\n",
            "Epoch 21, Training Loss: 1.0425, Validation Loss: 1.0858\n",
            "Epoch 22, Training Loss: 1.0366, Validation Loss: 1.1572\n",
            "Epoch 23, Training Loss: 1.0153, Validation Loss: 1.1572\n",
            "Epoch 24, Training Loss: 1.0310, Validation Loss: 1.1573\n",
            "Epoch 25, Training Loss: 1.0303, Validation Loss: 1.0867\n",
            "Saving model at epoch 26 with Validation Loss: 1.0665\n",
            "Epoch 26, Training Loss: 1.0134, Validation Loss: 1.0665\n",
            "Epoch 27, Training Loss: 0.9969, Validation Loss: 1.1229\n",
            "Saving model at epoch 28 with Validation Loss: 1.0423\n",
            "Epoch 28, Training Loss: 1.0079, Validation Loss: 1.0423\n",
            "Epoch 29, Training Loss: 1.0071, Validation Loss: 1.0799\n",
            "Epoch 30, Training Loss: 0.9910, Validation Loss: 1.1091\n",
            "Epoch 31, Training Loss: 0.9804, Validation Loss: 1.1084\n",
            "Saving model at epoch 32 with Validation Loss: 1.0235\n",
            "Epoch 32, Training Loss: 0.9877, Validation Loss: 1.0235\n",
            "Epoch 33, Training Loss: 0.9784, Validation Loss: 1.0711\n",
            "Epoch 34, Training Loss: 0.9740, Validation Loss: 1.1902\n",
            "Epoch 35, Training Loss: 0.9543, Validation Loss: 1.1048\n",
            "Epoch 36, Training Loss: 0.9766, Validation Loss: 1.0591\n",
            "Epoch 37, Training Loss: 0.9555, Validation Loss: 1.1590\n",
            "Epoch 00039: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Epoch 38, Training Loss: 0.9444, Validation Loss: 1.1884\n",
            "Epoch 39, Training Loss: 0.9275, Validation Loss: 1.1427\n",
            "Epoch 40, Training Loss: 0.9215, Validation Loss: 1.1412\n",
            "Epoch 41, Training Loss: 0.9368, Validation Loss: 1.1912\n",
            "Epoch 42, Training Loss: 0.9134, Validation Loss: 1.1415\n",
            "Epoch 43, Training Loss: 0.9063, Validation Loss: 1.1488\n",
            "Epoch 00045: reducing learning rate of group 0 to 5.0000e-06.\n",
            "Epoch 44, Training Loss: 0.9273, Validation Loss: 1.1550\n",
            "Epoch 45, Training Loss: 0.9213, Validation Loss: 1.1527\n",
            "Epoch 46, Training Loss: 0.8960, Validation Loss: 1.1518\n",
            "Epoch 47, Training Loss: 0.9131, Validation Loss: 1.1510\n",
            "Epoch 48, Training Loss: 0.9038, Validation Loss: 1.1480\n",
            "Epoch 49, Training Loss: 0.9262, Validation Loss: 1.1466\n",
            "Epoch 00051: reducing learning rate of group 0 to 5.0000e-07.\n",
            "Epoch 50, Training Loss: 0.9073, Validation Loss: 1.1419\n",
            "Epoch 51, Training Loss: 0.9101, Validation Loss: 1.1418\n",
            "Epoch 52, Training Loss: 0.9154, Validation Loss: 1.1419\n",
            "Epoch 53, Training Loss: 0.9068, Validation Loss: 1.1418\n",
            "Epoch 54, Training Loss: 0.9005, Validation Loss: 1.1424\n",
            "Epoch 55, Training Loss: 0.9128, Validation Loss: 1.1430\n",
            "Epoch 00057: reducing learning rate of group 0 to 5.0000e-08.\n",
            "Epoch 56, Training Loss: 0.9152, Validation Loss: 1.1429\n",
            "Epoch 57, Training Loss: 0.9069, Validation Loss: 1.1430\n",
            "Epoch 58, Training Loss: 0.9240, Validation Loss: 1.1429\n",
            "Epoch 59, Training Loss: 0.9075, Validation Loss: 1.1429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(box1, boxes2):\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    boxes2_area = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
        "\n",
        "    inter_x1 = torch.max(box1[0], boxes2[:, 0])\n",
        "    inter_y1 = torch.max(box1[1], boxes2[:, 1])\n",
        "    inter_x2 = torch.min(box1[2], boxes2[:, 2])\n",
        "    inter_y2 = torch.min(box1[3], boxes2[:, 3])\n",
        "\n",
        "    inter_area = torch.clamp((inter_x2 - inter_x1), min=0) * torch.clamp((inter_y2 - inter_y1), min=0)\n",
        "    union_area = box1_area + boxes2_area - inter_area\n",
        "\n",
        "    iou = inter_area / (union_area + 1e-6)  # Adding a small epsilon to avoid division by zero\n",
        "\n",
        "    return iou.cpu().numpy()\n",
        "\n",
        "def evaluate(model, val_loader, device):\n",
        "    model = model.to(device)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    all_labels50 = []\n",
        "    all_scores50 = []\n",
        "\n",
        "    all_labels75 = []\n",
        "    all_scores75 = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in val_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            for output, target in zip(outputs, targets):\n",
        "                labels = np.zeros(len(output['boxes']))\n",
        "                scores = output['scores'].cpu().numpy()\n",
        "                labels75 = labels.copy()\n",
        "\n",
        "                for box in target['boxes']:\n",
        "                    ious = compute_iou(box, output['boxes'])\n",
        "                    labels[ious > 0.5] = 1\n",
        "\n",
        "                all_labels50.append(labels)\n",
        "                all_scores50.append(scores)\n",
        "\n",
        "                for box in target['boxes']:\n",
        "                    ious = compute_iou(box, output['boxes'])\n",
        "                    labels75[ious > 0.95] = 1\n",
        "\n",
        "                all_labels75.append(labels75)\n",
        "                all_scores75.append(scores)\n",
        "\n",
        "    all_labels50 = np.concatenate(all_labels50)\n",
        "    all_scores50 = np.concatenate(all_scores50)\n",
        "\n",
        "    all_labels75 = np.concatenate(all_labels75)\n",
        "    all_scores75 = np.concatenate(all_scores75)\n",
        "\n",
        "    precision50, recall50, _ = precision_recall_curve(all_labels50, all_scores50)\n",
        "    precision75, recall75, _ = precision_recall_curve(all_labels75, all_scores75)\n",
        "    ap_50 = average_precision_score(all_labels50, all_scores50)\n",
        "    ap_75 = average_precision_score(all_labels75, all_scores75)\n",
        "\n",
        "    mAP = (ap_50 + ap_75) / 2\n",
        "\n",
        "    avg_precision = np.mean(precision50)\n",
        "    avg_recall = np.mean(recall50)\n",
        "\n",
        "    print(f'Precision: {avg_precision}')\n",
        "    print(f'Recall: {avg_recall}')\n",
        "    print(f'AP50: {ap_50}')\n",
        "    print(f'AP95: {ap_75}')\n",
        "    print(f'mAP: {mAP}')\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(recall50, precision50, marker='.')\n",
        "    plt.title('Precision-Recall curve')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.show()\n",
        "\n",
        "# Assume model, val_loader are already loaded/defined\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Call the evaluate function\n",
        "evaluate(model, val_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "TAFi2-TvqQFr",
        "outputId": "415fcbd9-1f32-4cc6-fa56-43c092146b7a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.12782352649884185\n",
            "Recall: 0.7961845730027549\n",
            "AP50: 0.2223488837338149\n",
            "AP95: -0.0\n",
            "mAP: 0.11117444186690745\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVSX6wEWOpQp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
